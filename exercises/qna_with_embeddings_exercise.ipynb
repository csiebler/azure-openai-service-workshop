{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Service - Q&A with semantic answering exerise\n",
    "\n",
    "In this tutorial, you'll build a simple Q&A system, that can give semantic answers to questions. Three sample documents from the Azure documentation are provided. Fill out the missing pieces in the source source to get everything working (indicated by `#FIXME`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "import openai\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from openai.embeddings_utils import cosine_similarity\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define embedding model and encoding\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "EMBEDDING_ENCODING = 'cl100k_base'\n",
    "EMBEDDING_CHUNK_SIZE = 8000\n",
    "COMPLETION_MODEL = 'text-davinci-003'\n",
    "\n",
    "# initialize tiktoken for encoding text\n",
    "encoding = tiktoken.get_encoding(EMBEDDING_ENCODING)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's read the documents in `samples/*.json`, which are our sample documents. The `content` section is the interesting piece of information for this tutorial:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"content\": \"\\n# What is Azure OpenAI?\\n\\nThe ...\",\n",
    "  \"product_name\": \"cognitive-services\",\n",
    "  \"title\": \"Azure Cognitive Services\",\n",
    "  \"description\": \"Apply advanced language models to variety of use cases with the Azure OpenAI service\",\n",
    "  \"topic\": \"overview\",\n",
    "  \"date\": \"11/07/2022\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in the samples directory\n",
    "samples_dir = os.path.join(os.getcwd(), \"data-qna/\")\n",
    "sample_files = os.listdir(samples_dir)\n",
    "\n",
    "# read content field from each file and append it to documents, and remove and newlines (better for embeddings later)\n",
    "documents = []\n",
    "for file in sample_files:\n",
    "    with open(os.path.join(samples_dir, file), \"r\") as f:\n",
    "        content = json.load(f)[\"content\"]\n",
    "        content = content.replace(\"\\n\", \" \")\n",
    "        content = content.replace(\"  \", \" \")\n",
    "        documents.append(content)\n",
    "\n",
    "# print some stats about the documents\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "for doc in documents:\n",
    "    num_tokens = len(encoding.encode(doc))\n",
    "    print(f\"Content: {doc[:80]}... \\n---> Tokens: {num_tokens}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all documents loaded, we can embed them using our embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for all docs\n",
    "embeddings = #FIXME\n",
    "\n",
    "# print some stats about the embeddings\n",
    "for e in embeddings:\n",
    "    print(len(e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our embeddings, we can try to ask some questions and see if it retrieves the correct document. You can try the following questions:\n",
    "\n",
    "* what is azure openai service?\n",
    "* can translator be fine tuned?\n",
    "* what is the difference between luis and clu?\n",
    "* what is form recognizer? (should yield no result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding for question\n",
    "question = \"what is azure openai service?\"\n",
    "qe = #FIXME\n",
    "\n",
    "# calculate cosine similarity between question and each document\n",
    "similaries = #FIXME\n",
    "\n",
    "# Get the matching document, in this case we just use argmax of similarities\n",
    "max_i = #FIXME\n",
    "\n",
    "# print some stats about the similarities\n",
    "for i, s in enumerate(similaries):\n",
    "    print(f\"Similarity to {sample_files[i]} is {s}\")\n",
    "print(f\"Matching document is {sample_files[max_i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a prompt that we use for completion, in this case we put the matched document and the question in the prompt\n",
    "prompt = #FIXME\n",
    "\n",
    "# get response from completion model\n",
    "response = #FIXME\n",
    "answer = #FIXME\n",
    "\n",
    "# print the question and answer\n",
    "print(f\"Question was: {question}\\nRetrieved answer was: {answer}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that worked. Now we should have a simple understanding how Q&A can work using OpenAI embeddings and completions. Next step would be:\n",
    "\n",
    "* Chunking of longer documents (you might run into token limits for embeddings and the answering prompt)\n",
    "* Usage of a vector database (pinecone, redis, etc.) to scale the search part to a larger amount of documents\n",
    "* Evaluation of the top k results, instead of just the best matching document\n",
    "* ...and a few more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-qna-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4ee1bbf3137c7ea9420c4fd488a55642063e5739fe2a7286130d9ba47405b69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
